{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13c2251",
   "metadata": {
    "id": "b13c2251"
   },
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e563313d",
   "metadata": {
    "id": "e563313d"
   },
   "source": [
    "# PyTorch Lightning Trainers\n",
    "\n",
    "In this tutorial, we demonstrate TorchGeo trainers to train and test a model. We will use the [EuroSAT](https://torchgeo.readthedocs.io/en/stable/api/datasets.html#eurosat) dataset throughout this tutorial. Specifically, a subset containing only 100 images. We will train models to predict land cover classes.\n",
    "\n",
    "It's recommended to run this notebook on Google Colab if you don't have your own GPU. Click the \"Open in Colab\" button above to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1f4156",
   "metadata": {
    "id": "8c1f4156"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, we install TorchGeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d31a8",
   "metadata": {
    "id": "3f0d31a8"
   },
   "outputs": [],
   "source": [
    "%pip install torchgeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c94c7",
   "metadata": {
    "id": "c90c94c7"
   },
   "source": [
    "## Imports\n",
    "\n",
    "Next, we import TorchGeo and any other libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd39f485",
   "metadata": {
    "id": "bd39f485"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torchgeo.datamodules import EuroSAT100DataModule\n",
    "from torchgeo.models import ResNet18_Weights\n",
    "from torchgeo.trainers import ClassificationTask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e1d9b6",
   "metadata": {
    "id": "e6e1d9b6"
   },
   "source": [
    "## Lightning modules\n",
    "\n",
    "Our trainers use [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/) to organize both the training code, and the dataloader setup code. This makes it easy to create and share reproducible experiments and results.\n",
    "\n",
    "First we'll create a `EuroSAT100DataModule` object which is simply a wrapper around the [EuroSAT100](https://torchgeo.readthedocs.io/en/latest/api/datasets.html#eurosat) dataset. This object 1.) ensures that the data is downloaded, 2.) sets up PyTorch `DataLoader` objects for the train, validation, and test splits, and 3.) ensures that data from the same region **is not** shared between the training and validation sets so that you can properly evaluate the generalization performance of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2daa0d",
   "metadata": {
    "id": "9f2daa0d"
   },
   "source": [
    "The following variables can be modified to control training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e100f8b",
   "metadata": {
    "nbmake": {
     "mock": {
      "batch_size": 1,
      "fast_dev_run": true,
      "max_epochs": 1,
      "num_workers": 0
     }
    },
    "id": "8e100f8b"
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_workers = 2\n",
    "max_epochs = 10\n",
    "fast_dev_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2a04c7",
   "metadata": {
    "id": "0f2a04c7"
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join(tempfile.gettempdir(), \"eurosat100\")\n",
    "datamodule = EuroSAT100DataModule(\n",
    "    root=data_dir, batch_size=batch_size, num_workers=num_workers, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056b7b4c",
   "metadata": {
    "id": "056b7b4c"
   },
   "source": [
    "Next, we create a `ClassificationTask` object that holds the model object, optimizer object, and training logic. We will use a ResNet-18 model that has been pre-trained on Sentinel-2 imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c5442",
   "metadata": {
    "id": "ba5c5442"
   },
   "outputs": [],
   "source": [
    "task = ClassificationTask(\n",
    "    loss=\"ce\",\n",
    "    model=\"resnet18\",\n",
    "    weights=ResNet18_Weights.SENTINEL2_ALL_MOCO,\n",
    "    in_channels=13,\n",
    "    num_classes=10,\n",
    "    learning_rate=0.1,\n",
    "    learning_rate_schedule_patience=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b67f3e",
   "metadata": {
    "id": "d4b67f3e"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have the Lightning modules set up, we can use a PyTorch Lightning [Trainer](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html) to run the training and evaluation loops. There are many useful pieces of configuration that can be set in the `Trainer` -- below we set up model checkpointing based on the validation loss, early stopping based on the validation loss, and a CSV based logger. We encourage you to see the [PyTorch Lightning docs](https://pytorch-lightning.readthedocs.io/) for other options that can be set here, e.g. Tensorboard logging, automatically selecting your optimizer's learning rate, and easy multi-GPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe26e5c",
   "metadata": {
    "id": "ffe26e5c"
   },
   "outputs": [],
   "source": [
    "experiment_dir = os.path.join(tempfile.gettempdir(), \"experiments\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", dirpath=experiment_dir, save_top_k=1, save_last=True\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=10)\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=experiment_dir, name=\"tutorial_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06afd8c7",
   "metadata": {
    "id": "06afd8c7"
   },
   "source": [
    "For tutorial purposes we deliberately lower the maximum number of training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a6d36",
   "metadata": {
    "id": "225a6d36"
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    logger=[csv_logger],\n",
    "    default_root_dir=experiment_dir,\n",
    "    min_epochs=1,\n",
    "    max_epochs=max_epochs,\n",
    "    fast_dev_run=fast_dev_run,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d71e8f",
   "metadata": {
    "id": "44d71e8f"
   },
   "source": [
    "When we first call `.fit(...)` the dataset will be downloaded and checksummed (if it hasn't already). This can take 5â€“10 minutes. After this, the training process will kick off, and results will be saved to a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e08790",
   "metadata": {
    "id": "00e08790"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model=task, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73700fb5",
   "metadata": {
    "id": "73700fb5"
   },
   "source": [
    "We load the log files and plot the training RMSE over batches, and the validation RMSE over epochs. We can see that our model is just starting to converge, and would probably benefit from additional training time and a lower initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38033e0",
   "metadata": {
    "id": "c38033e0"
   },
   "outputs": [],
   "source": [
    "if not fast_dev_run:\n",
    "    train_steps = []\n",
    "    train_rmse = []\n",
    "\n",
    "    val_steps = []\n",
    "    val_rmse = []\n",
    "    with open(\n",
    "        os.path.join(experiment_dir, \"tutorial_logs\", \"version_0\", \"metrics.csv\"), \"r\"\n",
    "    ) as f:\n",
    "        csv_reader = csv.DictReader(f, delimiter=\",\")\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            try:\n",
    "                train_rmse.append(float(row[\"train_RMSE\"]))\n",
    "                train_steps.append(i)\n",
    "            except ValueError:  # Ignore rows where train RMSE is empty\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                val_rmse.append(float(row[\"val_RMSE\"]))\n",
    "                val_steps.append(i)\n",
    "            except ValueError:  # Ignore rows where val RMSE is empty\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba23407",
   "metadata": {
    "id": "1ba23407"
   },
   "outputs": [],
   "source": [
    "if not fast_dev_run:\n",
    "    plt.figure()\n",
    "    plt.plot(train_steps, train_rmse, label=\"Train RMSE\")\n",
    "    plt.plot(val_steps, val_rmse, label=\"Validation RMSE\")\n",
    "    plt.legend(fontsize=15)\n",
    "    plt.xlabel(\"Batches\", fontsize=15)\n",
    "    plt.ylabel(\"RMSE\", fontsize=15)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cfc7a8",
   "metadata": {
    "id": "04cfc7a8"
   },
   "source": [
    "Finally, after the model has been trained, we can easily evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a3b2f",
   "metadata": {
    "id": "604a3b2f"
   },
   "outputs": [],
   "source": [
    "trainer.test(model=task, datamodule=datamodule)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "execution": {
   "timeout": 1200
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}